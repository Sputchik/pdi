from sputchedtools import aio, enhance_loop
from bs4 import BeautifulSoup

import aiohttp, aiofiles, asyncio, ssl, ua_generator, json

github_latest_draft = 'https://api.github.com/repos/{}/{}/releases/latest' # Owner, Repo Slug
urls_link = 'https://raw.githubusercontent.com/Sputchik/program-downloader/refs/heads/main/urls.txt'

RegistryFinder = 'https://registry-finder.com/'
Go = 'https://go.dev/dl/?mode=json'
Google_Earth_Pro = 'https://support.google.com/earth/answer/168344?hl=en#zippy=%2Cdownload-a-google-earth-pro-direct-installer'
Git = 'https://git-scm.com/downloads/win'
Wireless_Bluetooth = 'https://www.intel.com/content/www/us/en/download/18649/intel-wireless-bluetooth-drivers-for-windows-10-and-windows-11.html'
Git = 'https://git-scm.com/downloads/win'
Python = 'https://www.python.org/downloads/'
Nodejs = 'https://nodejs.org/en'
NVCleanstall = 'https://nvcleanstall.net/download'
KLiteCodec = 'https://www.codecguide.com/download_k-lite_codec_pack_standard.htm'
Everything = 'https://www.voidtools.com/'
qBitTorrent = 'https://www.qbittorrent.org/download'

def get_line_index(lines, start_pattern):
	for index, line in enumerate(lines):
		if line.startswith(start_pattern):
			return index

def parse_categories(lines):
	cat_map = {}
	cat_index = get_line_index(lines, 'Categories=')
	categories = lines[cat_index].split('=')[1].split(';')

	cat_progs_start = get_line_index(lines, categories[0])
	cat_progs_end = get_line_index(lines, categories[-1])

	progs_lines = lines[cat_progs_start:cat_progs_end]
	for line in progs_lines:
		cat, progs = line.split('=')
		progs = sorted(progs.split(';'))
		cat_map[cat] = ';'.join(progs)
	
	return cat_map

async def parse_github_urls() -> dict:
	response = await aio.request(urls_link, toreturn = 'text+ok')
	data, ok = response

	if not ok:
		print(f'Fail: Github urls fetch: {urls_link}')
		return
	
	lines: list[str] = data.splitlines()
	# print(*lines, sep = '\n')
	url_index = get_line_index(lines, 'url_')
	url_lines = lines[url_index:]
	print(url_lines)
	progmap = {
		'cats': parse_categories(lines),
		'urls': dict(sorted({
			line.split('url_')[1].split('=')[0].replace('^', ' '): line.split('=', maxsplit = 1)[1] for line in url_lines
		}.items(), key = lambda x: x.casefold()))
	}

	return progmap

def progmap_to_txt(progmap):
	first_line = 'Categories=' + ';'.join(list(progmap['cats'].keys()))
	cat_progs = '\n'.join([f"{key}={value}" for key, value in progmap['cats'].items()])
	urls = '\n'.join([f"url_{key.replace(' ', '^')}={value}" for key, value in progmap['urls'].items()])
	result = '\n\n'.join((first_line, cat_progs, urls))
	return result

# async def parse_github_urls() -> dict:
# 	response = await aio.request(urls_link, toreturn = 'text+ok')
# 	data, ok = response

# 	if not ok:
# 		print(f'Fail: Github urls fetch: {urls_link}')
# 		return

# 	lines: list[str] = data.splitlines()
# 	# print(*lines, sep = '\n')

# 	progmap = {
# 		'version': lines[0],
# 		'urls': {
# 			line.split('url_')[1].split('=')[0].replace('^', ' '): line.split('=', maxsplit = 1)[1] for line in lines[get_url_line_inex(lines):]}
# 	}

# 	return progmap

def extract_versions(versions: dict[str, str]) -> str:
	preferred_exe = None
	selected_exe = False
	preferred_msi = None

	for key in versions:
		if key.endswith('.msi') and 'arm' not in key:
			# Check if it's a preferred '64' version
			if '64' in key:
				preferred_msi = key
				break
			elif '32' in key and not preferred_msi:
				preferred_msi = key
			elif not preferred_msi:
				preferred_msi = key

		elif key.endswith('.exe') and 'arm' not in key:
			# Check if it's a preferred '64' version
			if '64' in key and not selected_exe:
				preferred_exe = key
				selected_exe = True
			elif '32' in key and not preferred_exe:
				preferred_exe = key
			elif not preferred_exe:
				preferred_exe = key

	preferred_key = preferred_msi if preferred_msi else preferred_exe
	return preferred_key

async def direct_from_github(owner: str, project: str) -> str:
	url = github_latest_draft.format(owner, project)

	response = await aio.request(
		url,
		toreturn = 'json+ok'
	)
	data, ok = response

	if not ok or not isinstance(data, dict) or 'assets' not in data:
		print(f'Fail: Github latest version for `{project}`: {url}')
		return

	assets = data['assets']
	version_map = {unpack['name']: unpack['browser_download_url'] for unpack in assets}
	key = extract_versions(version_map)

	if not key:
		print(f'Fail: Github key version extraction: {url}')

	# print(version_map[key])
	return version_map[key]

async def parse(url, name):

	if name == 'ZXP Installer':
		author = 'elements-storage'
		project = 'ZXPInstaller'
		return await direct_from_github(author, project)

	elif name == '7-Zip':
		author = 'ip7z'
		project = '7zip'
		return await direct_from_github(author, project)

	elif name == 'VCRedist_2005-2022':
		author = 'abbodi1406'
		project = 'vcredist'
		return await direct_from_github(author, project)

	elif name == 'Git':
		author = 'git-for-windows'
		project = 'git'
		return await direct_from_github(author, project)
	
	elif name == 'ContextMenuManager':
		author = 'BluePointLilac'
		return await direct_from_github(author, name)
	
	elif name == 'Rufus':
		author = 'pbatard'
		project = 'rufus'
		return await direct_from_github(author, project)
	
	elif name == 'OBS':
		author = 'obsproject'
		project = 'obs-studio'
		return await direct_from_github(author, project)

	# headers = {'User-Agent': ua_generator.generate().text}
	response = await aio.request(url, toreturn = 'text+status')
	data, status = response

	print(f'{status}: {name} - {url}')
	if status != 200:
		return

	if name == 'Go':
		version = json.loads(data)[0]['version'].split('go')[1]
		return f'https://go.dev/dl/go{version}.windows-amd64.msi'


	soup = BeautifulSoup(data, 'lxml')

	if name == 'RegistryFinder':
		for a_tag in soup.find_all('a'):
			href = a_tag.get('href')
			if href and href.startswith('bin/'):
				return f'https://registry-finder.com/{href}'

	elif name == 'Google Earth Pro':
		lis = soup.find_all('li')

		for li in lis:
			if li.text and 'for Windows (64-bit)' in li.text:
				return li.find('a').get('href')

	elif name == 'Wireless Bluetooth':
		button = soup.find('button', {'data-wap_ref': 'download-button'})
		url = button.get('data-href')
		return url

	elif name == 'Gradle':
		div = soup.find('div', class_ = 'resources-contents')
		version = div.find('a').get('name')
		return f'https://services.gradle.org/distributions/gradle-{version}-bin.zip'

	elif name == 'Python':
		a = soup.find('a', class_ = 'button')
		print(a)
		version = a.text.split(' ')[2]

		return f'https://www.python.org/ftp/python/{version}/python-{version}-amd64.exe'

	elif name == 'Node.js':
		a_elems = soup.find_all('b')

		for elem in a_elems:
			a = elem.find('a')
			if a:
				version = a.text
				return  f'https://nodejs.org/dist/{version}/node-{version}-x64.msi'

	elif name == 'NVCleanstall':
		a = soup.find('a', class_ = 'btn btn btn-info my-5')
		return a.get('href')
	
	elif name == 'K-Lite Codec':
		a_elems = soup.find_all('a')

		for elem in a_elems:
			if elem.text and elem.text == 'Server 2':
				return elem.get('href')
	
	elif name == 'Everything':
		a_elems = soup.find_all('a', class_ = 'button')
		
		for elem in a_elems:
			if elem.text and elem.text.endswith('64-bit'):
				url = elem.get('href')
				return f'https://www.voidtools.com{url}'
		
	elif name == 'qBitTorrent':
		a_elems = soup.find_all('a')

		for elem in a_elems:
			if elem.text and elem.text.startswith('Download qBittorrent '):
				version = elem.text.split(' ')[2]
				return f'https://netcologne.dl.sourceforge.net/project/qbittorrent/qbittorrent-win32/qbittorrent-{version}/qbittorrent_{version}_x64_setup.exe?viasf=1'

async def main():
	progmap = await parse_github_urls()
	#print(json.dumps(progmap, indent = 2))
	print(progmap_to_txt(progmap))

if __name__ == '__main__':
	enhance_loop()
	asyncio.run(main())